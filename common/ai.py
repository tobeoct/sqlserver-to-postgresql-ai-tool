
import openai
from common.config import Config

class PromptError(Exception):
    def __init__(self, message):
        self.message = message
        super().__init__(self.message)

class ChatGPTModels:
    GPT_3_5_TURBO_INSTRUCT ='gpt-3.5-turbo-instruct'

class LLM:
    def estimate_tokens(self,prompt) -> None:
        pass
    def ask(self,prompt, max_tokens) -> None:
        pass
    def ask_if_valid(self,prompt) -> None:
        pass
    def ask_if_valid(self,prompt,estimated_completion_tokens)-> None:
        pass

class ChatGPT(LLM):
    def __init__(self, model):
        self.client = openai.OpenAI(api_key=Config.OPEN_AI_API_KEY)
        self.model = model
        self.model_max_tokens = 4096
    
    def __validate_prompt_tokens(self, prompt_tokens, completion_tokens):
        estimated_total_tokens = prompt_tokens + completion_tokens
        print(f"Estimated Total number of tokens:{estimated_total_tokens}")
        if estimated_total_tokens > self.model_max_tokens:
            raise PromptError("WARN: Request would exceed token quota, skipping request") 
        
        
    def __validate_prompt(self,prompt):
        prompt_tokens = self.estimate_tokens(prompt)
        self.__validate_prompt_tokens(prompt_tokens, prompt_tokens) 
        return prompt_tokens
    
    def estimate_tokens(self,prompt):
        response = self.client.completions.create(
            model=self.model,
            prompt=prompt,
            max_tokens=1,
            n=1,
            stop=None,
            temperature=0.5
        )
        
        return response.usage.prompt_tokens

    def ask(self,prompt, max_tokens):
        response = self.client.completions.create(
            model=self.model,
            prompt=prompt,
            max_tokens=max_tokens,
            n=1,
            stop=None,
            temperature=0.5
        )

        return response.choices[0].text.strip()
    
    def ask_if_valid(self,prompt):
        prompt_token_length = self.__validate_prompt(prompt)
        return self.ask(prompt, prompt_token_length)
    
    def ask_if_valid(self,prompt,estimated_completion_tokens):
        """
        Completion tokens are the estimated number of response tokens. This is generated by tokenizing the expected response from the LLM
        """
        prompt_tokens = self.estimate_tokens(prompt)
        self.__validate_prompt_tokens(prompt_tokens, estimated_completion_tokens)
        return self.ask(prompt, prompt_tokens)
    



class AI:
    def __init__(self, llm: LLM):
        self.llm = llm
    
    def estimate_tokens(self,prompt):
        self.llm.estimate_tokens(prompt)

    def ask(self,prompt, max_tokens):
        return self.llm.ask(prompt,max_tokens)
    
    def ask_if_valid(self,prompt):
        return self.llm.ask_if_valid(prompt)
    
    def ask_if_valid(self,prompt,estimated_completion_tokens):
        """
        Completion tokens are the estimated number of response tokens. This is generated by tokenizing the expected response from the LLM
        """
        return self.llm.ask_if_valid(prompt,estimated_completion_tokens)